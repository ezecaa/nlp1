{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfa39F4lsLf3"
   },
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "## LSTM Traductor\n",
    "## Trábajo Práctico Nro. 4\n",
    "## Alumno: Ezequiel Alejandro Caamaño\n",
    "## a1802\n",
    "Ejemplo basado en [LINK](https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqO0PRcFsPTe"
   },
   "source": [
    "### Datos\n",
    "El objecto es utilizar datos disponibles de Anki de traducciones de texto en diferentes idiomas. Se construirá un modelo traductor seq2seq utilizando encoder-decoder.\\\n",
    "[LINK](https://www.manythings.org/anki/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FGmtK8J19PNk"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --no-cache-dir gdown --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cq3YXak9sGHd"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vgYatMIdk_eT",
    "outputId": "6b6cfc9b-7848-491b-d490-90bd9545948f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OYpIWGaXxfKe",
    "outputId": "aead9aa5-59a1-48ba-a729-c1e926263024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "# torchsummar actualmente tiene un problema con las LSTM, por eso\n",
    "# se utiliza torchinfo, un fork del proyecto original con el bug solucionado\n",
    "!pip3 install torchinfo\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GHFPS5KNxgR9",
    "outputId": "48cf26c4-1e12-42e8-fe37-60d6a99c37d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-18 09:47:57--  http://torch_helpers.py/\n",
      "Resolving torch_helpers.py (torch_helpers.py)... failed: Name or service not known.\n",
      "wget: unable to resolve host address ‘torch_helpers.py’\n",
      "--2025-06-18 09:47:58--  https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 23883 (23K) [text/plain]\n",
      "Saving to: ‘torch_helpers.py’\n",
      "\n",
      "torch_helpers.py    100%[===================>]  23.32K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-06-18 09:47:58 (61.1 MB/s) - ‘torch_helpers.py’ saved [23883/23883]\n",
      "\n",
      "FINISHED --2025-06-18 09:47:58--\n",
      "Total wall clock time: 0.5s\n",
      "Downloaded: 1 files, 23K in 0s (61.1 MB/s)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "\n",
    "if os.access('torch_helpers.py', os.F_OK) is False:\n",
    "    if platform.system() == 'Windows':\n",
    "        !curl !wget https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py > torch_helpers.py\n",
    "    else:\n",
    "        !wget torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tP-fbmHUgbtp"
   },
   "outputs": [],
   "source": [
    "def sequence_acc(y_pred, y_test):\n",
    "    y_pred_tag = y_pred.data.max(dim=-1,keepdim=True)[1]\n",
    "    y_test_tag = y_test.data.max(dim=-1,keepdim=True)[1]\n",
    "\n",
    "    batch_size = y_pred_tag.shape[0]\n",
    "    batch_acc = torch.zeros(batch_size)\n",
    "    for b in range(batch_size):\n",
    "        correct_results_sum = (y_pred_tag[b] == y_test_tag[b]).sum().float()\n",
    "        batch_acc[b] = correct_results_sum / y_pred_tag[b].shape[0]\n",
    "\n",
    "    correct_results_sum = batch_acc.sum().float()\n",
    "    acc = correct_results_sum / batch_size\n",
    "    return acc\n",
    "\n",
    "def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100):\n",
    "    # Defino listas para realizar graficas de los resultados\n",
    "    train_loss = []\n",
    "    train_accuracy = []\n",
    "    valid_loss = []\n",
    "    valid_accuracy = []\n",
    "\n",
    "    # Defino mi loop de entrenamiento\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        epoch_train_loss = 0.0\n",
    "        epoch_train_accuracy = 0.0\n",
    "\n",
    "        for train_encoder_input, train_decoder_input, train_target in train_loader:\n",
    "            # Seteo los gradientes en cero ya que, por defecto, PyTorch\n",
    "            # los va acumulando\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(train_encoder_input.to(device), train_decoder_input.to(device))\n",
    "\n",
    "            # Computo el error de la salida comparando contra las etiquetas\n",
    "            # por cada token en cada batch (sequence_loss)\n",
    "            loss = 0\n",
    "            for t in range(train_decoder_input.shape[1]):\n",
    "                loss += criterion(output[:, t, :], train_target[:, t, :])\n",
    "\n",
    "            # Almaceno el error del batch para luego tener el error promedio de la epoca\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "            # Computo el nuevo set de gradientes a lo largo de toda la red\n",
    "            loss.backward()\n",
    "\n",
    "            # Realizo el paso de optimizacion actualizando los parametros de toda la red\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculo el accuracy del batch\n",
    "            accuracy = sequence_acc(output, train_target)\n",
    "            # Almaceno el accuracy del batch para luego tener el accuracy promedio de la epoca\n",
    "            epoch_train_accuracy += accuracy.item()\n",
    "\n",
    "        # Calculo la media de error para la epoca de entrenamiento.\n",
    "        # La longitud de train_loader es igual a la cantidad de batches dentro de una epoca.\n",
    "        epoch_train_loss = epoch_train_loss / len(train_loader)\n",
    "        train_loss.append(epoch_train_loss)\n",
    "        epoch_train_accuracy = epoch_train_accuracy / len(train_loader)\n",
    "        train_accuracy.append(epoch_train_accuracy)\n",
    "\n",
    "        # Realizo el paso de validación computando error y accuracy, y\n",
    "        # almacenando los valores para imprimirlos y graficarlos\n",
    "        valid_encoder_input, valid_decoder_input, valid_target = next(iter(valid_loader))\n",
    "        output = model(valid_encoder_input.to(device), valid_decoder_input.to(device))\n",
    "\n",
    "        epoch_valid_loss = 0\n",
    "        for t in range(train_decoder_input.shape[1]):\n",
    "                epoch_valid_loss += criterion(output[:, t, :], valid_target[:, t, :])\n",
    "        epoch_valid_loss = epoch_valid_loss.item()\n",
    "\n",
    "        valid_loss.append(epoch_valid_loss)\n",
    "\n",
    "        # Calculo el accuracy de la epoch\n",
    "        epoch_valid_accuracy = sequence_acc(output, valid_target).item()\n",
    "        valid_accuracy.append(epoch_valid_accuracy)\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Train accuracy {epoch_train_accuracy:.3f} - Valid Loss {epoch_valid_loss:.3f} - Valid accuracy {epoch_valid_accuracy:.3f}\")\n",
    "\n",
    "    history = {\n",
    "        \"loss\": train_loss,\n",
    "        \"accuracy\": train_accuracy,\n",
    "        \"val_loss\": valid_loss,\n",
    "        \"val_accuracy\": valid_accuracy,\n",
    "    }\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BFiCH8nxoIY"
   },
   "source": [
    "### 1 - Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RHNkUaPp6aYq",
    "outputId": "e485bd93-3f2f-4c9b-bac6-698d53e1f73e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
      "To: /content/spa-eng.zip\n",
      "100%|██████████| 2.64M/2.64M [00:00<00:00, 233MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Descargar la carpeta de dataset\n",
    "import os\n",
    "import gdown\n",
    "if os.access('spa-eng', os.F_OK) is False:\n",
    "    if os.access('simpsons_dataset.zip', os.F_OK) is False:\n",
    "        url = 'http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip'\n",
    "        output = 'spa-eng.zip'\n",
    "        gdown.download(url, output, quiet=False)\n",
    "    !unzip -q spa-eng.zip\n",
    "else:\n",
    "    print(\"El dataset ya se encuentra descargado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-9aNLZBDtA5J",
    "outputId": "b90aa703-d62f-4dee-e963-c6e01d7403af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de rows disponibles: 118964\n",
      "Cantidad de rows utilizadas: 6000\n"
     ]
    }
   ],
   "source": [
    "# dataset_file\n",
    "\n",
    "text_file = \"./spa-eng/spa.txt\"\n",
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "# Por limitaciones de RAM no se leen todas las filas\n",
    "MAX_NUM_SENTENCES = 6000\n",
    "\n",
    "# Mezclar el dataset, forzar semilla siempre igual\n",
    "np.random.seed([40])\n",
    "np.random.shuffle(lines)\n",
    "\n",
    "input_sentences = []\n",
    "output_sentences = []\n",
    "output_sentences_inputs = []\n",
    "count = 0\n",
    "\n",
    "for line in lines:\n",
    "    count += 1\n",
    "    if count > MAX_NUM_SENTENCES:\n",
    "        break\n",
    "\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "\n",
    "    # Input sentence --> eng\n",
    "    # output --> spa\n",
    "    input_sentence, output = line.rstrip().split('\\t')\n",
    "\n",
    "    # output sentence (decoder_output) tiene <eos>\n",
    "    output_sentence = output + ' <eos>'\n",
    "    # output sentence input (decoder_input) tiene <sos>\n",
    "    output_sentence_input = '<sos> ' + output\n",
    "\n",
    "    input_sentences.append(input_sentence)\n",
    "    output_sentences.append(output_sentence)\n",
    "    output_sentences_inputs.append(output_sentence_input)\n",
    "\n",
    "print(\"Cantidad de rows disponibles:\", len(lines))\n",
    "print(\"Cantidad de rows utilizadas:\", len(input_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93IGMKFb73q7",
    "outputId": "671921fd-296c-49b2-d867-9b3c97f54dd3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A deal is a deal.',\n",
       " 'Un trato es un trato. <eos>',\n",
       " '<sos> Un trato es un trato.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentences[0], output_sentences[0], output_sentences_inputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8P-ynUNP5xp6"
   },
   "source": [
    "### 2 - Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5WAZGOTfGyha"
   },
   "outputs": [],
   "source": [
    "# Definir el tamaño máximo del vocabulario\n",
    "MAX_VOCAB_SIZE = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eF1W6peoFGXA",
    "outputId": "3e873ec6-2691-4b36-d351-3a658a253f53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras en el vocabulario: 3851\n",
      "Sentencia de entrada más larga: 32\n"
     ]
    }
   ],
   "source": [
    "# Tokenizar las palabras con el Tokenizer de Keras\n",
    "# Definir una máxima cantidad de palabras a utilizar:\n",
    "# - num_words --> the maximum number of words to keep, based on word frequency.\n",
    "# - Only the most common num_words-1 words will be kept.\n",
    "from torch_helpers import Tokenizer\n",
    "input_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "input_tokenizer.fit_on_texts(input_sentences)\n",
    "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
    "\n",
    "word2idx_inputs = input_tokenizer.word_index\n",
    "print(\"Palabras en el vocabulario:\", len(word2idx_inputs))\n",
    "\n",
    "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
    "print(\"Sentencia de entrada más larga:\", max_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zBzdKiTVIBYY",
    "outputId": "25a1ab2a-f04e-431f-ec64-3d9ba360cbfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras en el vocabulario: 5721\n",
      "Sentencia de salida más larga: 36\n"
     ]
    }
   ],
   "source": [
    "# A los filtros de símbolos del Tokenizer agregamos el \"¿\",\n",
    "# sacamos los \"<>\" para que no afectar nuestros tokens\n",
    "output_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='!\"#$%&()*+,-./:;=¿?@[\\\\]^_`{|}~\\t\\n')\n",
    "output_tokenizer.fit_on_texts([\"<sos>\", \"<eos>\"] + output_sentences)\n",
    "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
    "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
    "\n",
    "word2idx_outputs = output_tokenizer.word_index\n",
    "print(\"Palabras en el vocabulario:\", len(word2idx_outputs))\n",
    "\n",
    "num_words_output = min(len(word2idx_outputs) + 1, MAX_VOCAB_SIZE) # Se suma 1 por el primer <sos>\n",
    "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
    "print(\"Sentencia de salida más larga:\", max_out_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xqb8ZJ4sJHgv"
   },
   "source": [
    "Como era de esperarse, las sentencias en castellano son más largas que en inglés, y lo mismo sucede con su vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "pgLC706EQx3p"
   },
   "outputs": [],
   "source": [
    "# Por una cuestion de que no explote la RAM se limitará el tamaño de las sentencias de entrada\n",
    "# a la mitad:\n",
    "max_input_len = 16\n",
    "max_out_len = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGOn9N57IuYz"
   },
   "source": [
    "A la hora de realiza padding es importante teneer en cuenta que en el encoder los ceros se agregan al comienoz y en el decoder al final. Esto es porque la salida del encoder está basado en las últimas palabras de la sentencia (son las más importantes), mientras que en el decoder está basado en el comienzo de la secuencia de salida ya que es la realimentación del sistema y termina con fin de sentencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q0Ob4hAWJkcv",
    "outputId": "3d586d43-daf4-4b0a-8ee7-6c56865ea922"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de rows del dataset: 6000\n",
      "encoder_input_sequences shape: (6000, 16)\n",
      "decoder_input_sequences shape: (6000, 18)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np # Importa numpy si lo necesitas para otras operaciones\n",
    "print(\"Cantidad de rows del dataset:\", len(input_integer_seq))\n",
    "\n",
    "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
    "print(\"encoder_input_sequences shape:\", encoder_input_sequences.shape)\n",
    "\n",
    "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_input_sequences shape:\", decoder_input_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3VySR1pzx9UG",
    "outputId": "47c671e8-e250-4c21-d7d3-f46937dd67bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_output_sequences shape: (6000, 18)\n"
     ]
    }
   ],
   "source": [
    "decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_output_sequences shape:\", decoder_output_sequences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wK4blEEsRQv3"
   },
   "source": [
    "La última capa del modelo (softmax) necesita que los valores de salida\n",
    "del decoder (decoder_sequences) estén en formato oneHotEncoder.\\\n",
    "Se utiliza \"decoder_output_sequences\" con la misma estrategía que se transformó la entrada del decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ANTOqJ0WWw-q",
    "outputId": "ceba39b5-770f-4ce3-d7bb-086cb49ebf18"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6000, 18])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(decoder_output_sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SD0bpM32yWfB",
    "outputId": "86c2e7a3-d357-4726-baca-8133329e4989"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_size: 16\n",
      "decoder_input_size: 18\n",
      "Output dim 5722\n"
     ]
    }
   ],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, encoder_input, decoder_input, decoder_output):\n",
    "        # Convertir los arrays de numpy a tensores.\n",
    "        # pytorch espera en general entradas 32bits\n",
    "        self.encoder_inputs = torch.from_numpy(encoder_input.astype(np.int32))\n",
    "        self.decoder_inputs = torch.from_numpy(decoder_input.astype(np.int32))\n",
    "        # Transformar los datos a oneHotEncoding\n",
    "        # la loss function esperan la salida float\n",
    "        self.decoder_outputs = F.one_hot(torch.from_numpy(decoder_output).to(torch.int64), num_classes=num_words_output).float()\n",
    "\n",
    "        self.len = self.decoder_outputs.shape[0]\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.encoder_inputs[index], self.decoder_inputs[index], self.decoder_outputs[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "data_set = Data(encoder_input_sequences, decoder_input_sequences, decoder_output_sequences)\n",
    "\n",
    "encoder_input_size = data_set.encoder_inputs.shape[1]\n",
    "print(\"encoder_input_size:\", encoder_input_size)\n",
    "\n",
    "decoder_input_size = data_set.decoder_inputs.shape[1]\n",
    "print(\"decoder_input_size:\", decoder_input_size)\n",
    "\n",
    "output_dim = data_set.decoder_outputs.shape[2]\n",
    "print(\"Output dim\", output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sUDPZeuAU1RI",
    "outputId": "bc56ccc6-aa58-4204-d165-8507c937f1a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: 4800\n",
      "Tamaño del conjunto de validacion: 1200\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "valid_set_size = int(data_set.len * 0.2)\n",
    "train_set_size = data_set.len - valid_set_size\n",
    "\n",
    "train_set = torch.utils.data.Subset(data_set, range(train_set_size))\n",
    "valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len))\n",
    "\n",
    "print(\"Tamaño del conjunto de entrenamiento:\", len(train_set))\n",
    "print(\"Tamaño del conjunto de validacion:\", len(valid_set))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CJIsLBbj6rg"
   },
   "source": [
    "### 3 - Preparar los embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9OcT-DLzkHS8",
    "outputId": "18ffb7fe-a0b6-4cd4-f71f-8629c47ddf10"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1wlDBOrxPq2-3htQ6ryVo7K1XnzLcfh4r&export=download\n",
      "From (redirected): https://drive.google.com/uc?id=1wlDBOrxPq2-3htQ6ryVo7K1XnzLcfh4r&export=download&confirm=t&uuid=4a717139-172f-4b1c-bcb2-08c8a73e71da\n",
      "To: /content/gloveembedding.pkl\n",
      "100%|██████████| 525M/525M [00:05<00:00, 97.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Descargar los embeddings desde un google drive (es la forma más rápida)\n",
    "# NOTA: No hay garantía de que estos links perduren, en caso de que no estén\n",
    "# disponibles descargar de la página oficial como se explica en el siguiente bloque\n",
    "import os\n",
    "import gdown\n",
    "if os.access('gloveembedding.pkl', os.F_OK) is False:\n",
    "    url = 'https://drive.google.com/uc?id=1wlDBOrxPq2-3htQ6ryVo7K1XnzLcfh4r&export=download'\n",
    "    output = 'gloveembedding.pkl'\n",
    "    gdown.download(url, output, quiet=False)\n",
    "else:\n",
    "    print(\"Los embeddings gloveembedding.pkl ya están descargados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ZgqtV8GpkSc8"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "import pickle\n",
    "\n",
    "class WordsEmbeddings(object):\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    def __init__(self):\n",
    "        # load the embeddings\n",
    "        words_embedding_pkl = Path(self.PKL_PATH)\n",
    "        if not words_embedding_pkl.is_file():\n",
    "            words_embedding_txt = Path(self.WORD_TO_VEC_MODEL_TXT_PATH)\n",
    "            assert words_embedding_txt.is_file(), 'Words embedding not available'\n",
    "            embeddings = self.convert_model_to_pickle()\n",
    "        else:\n",
    "            embeddings = self.load_model_from_pickle()\n",
    "        self.embeddings = embeddings\n",
    "        # build the vocabulary hashmap\n",
    "        index = np.arange(self.embeddings.shape[0])\n",
    "        # Dicctionarios para traducir de embedding a IDX de la palabra\n",
    "        self.word2idx = dict(zip(self.embeddings['word'], index))\n",
    "        self.idx2word = dict(zip(index, self.embeddings['word']))\n",
    "\n",
    "    def get_words_embeddings(self, words):\n",
    "        words_idxs = self.words2idxs(words)\n",
    "        return self.embeddings[words_idxs]['embedding']\n",
    "\n",
    "    def words2idxs(self, words):\n",
    "        return np.array([self.word2idx.get(word, -1) for word in words])\n",
    "\n",
    "    def idxs2words(self, idxs):\n",
    "        return np.array([self.idx2word.get(idx, '-1') for idx in idxs])\n",
    "\n",
    "    def load_model_from_pickle(self):\n",
    "        self.logger.debug(\n",
    "            'loading words embeddings from pickle {}'.format(\n",
    "                self.PKL_PATH\n",
    "            )\n",
    "        )\n",
    "        max_bytes = 2**28 - 1 # 256MB\n",
    "        bytes_in = bytearray(0)\n",
    "        input_size = os.path.getsize(self.PKL_PATH)\n",
    "        with open(self.PKL_PATH, 'rb') as f_in:\n",
    "            for _ in range(0, input_size, max_bytes):\n",
    "                bytes_in += f_in.read(max_bytes)\n",
    "        embeddings = pickle.loads(bytes_in)\n",
    "        self.logger.debug('words embeddings loaded')\n",
    "        return embeddings\n",
    "\n",
    "    def convert_model_to_pickle(self):\n",
    "        # create a numpy strctured array:\n",
    "        # word     embedding\n",
    "        # U50      np.float32[]\n",
    "        # word_1   a, b, c\n",
    "        # word_2   d, e, f\n",
    "        # ...\n",
    "        # word_n   g, h, i\n",
    "        self.logger.debug(\n",
    "            'converting and loading words embeddings from text file {}'.format(\n",
    "                self.WORD_TO_VEC_MODEL_TXT_PATH\n",
    "            )\n",
    "        )\n",
    "        structure = [('word', np.dtype('U' + str(self.WORD_MAX_SIZE))),\n",
    "                     ('embedding', np.float32, (self.N_FEATURES,))]\n",
    "        structure = np.dtype(structure)\n",
    "        # load numpy array from disk using a generator\n",
    "        with open(self.WORD_TO_VEC_MODEL_TXT_PATH, encoding=\"utf8\") as words_embeddings_txt:\n",
    "            embeddings_gen = (\n",
    "                (line.split()[0], line.split()[1:]) for line in words_embeddings_txt\n",
    "                if len(line.split()[1:]) == self.N_FEATURES\n",
    "            )\n",
    "            embeddings = np.fromiter(embeddings_gen, structure)\n",
    "        # add a null embedding\n",
    "        null_embedding = np.array(\n",
    "            [('null_embedding', np.zeros((self.N_FEATURES,), dtype=np.float32))],\n",
    "            dtype=structure\n",
    "        )\n",
    "        embeddings = np.concatenate([embeddings, null_embedding])\n",
    "        # dump numpy array to disk using pickle\n",
    "        max_bytes = 2**28 - 1 # # 256MB\n",
    "        bytes_out = pickle.dumps(embeddings, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(self.PKL_PATH, 'wb') as f_out:\n",
    "            for idx in range(0, len(bytes_out), max_bytes):\n",
    "                f_out.write(bytes_out[idx:idx+max_bytes])\n",
    "        self.logger.debug('words embeddings loaded')\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class GloveEmbeddings(WordsEmbeddings):\n",
    "    WORD_TO_VEC_MODEL_TXT_PATH = 'glove.twitter.27B.50d.txt'\n",
    "    PKL_PATH = 'gloveembedding.pkl'\n",
    "    N_FEATURES = 50\n",
    "    WORD_MAX_SIZE = 60\n",
    "\n",
    "class FasttextEmbeddings(WordsEmbeddings):\n",
    "    WORD_TO_VEC_MODEL_TXT_PATH = 'cc.en.300.vec'\n",
    "    PKL_PATH = 'fasttext.pkl'\n",
    "    N_FEATURES = 300\n",
    "    WORD_MAX_SIZE = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Mosj2-x-kXBK"
   },
   "outputs": [],
   "source": [
    "# Por una cuestion de RAM se utilizará los embeddings de Glove de dimension 50\n",
    "model_embeddings = GloveEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9FS8ca1ke_B",
    "outputId": "c681aea0-3f79-467f-ba8d-ccd49c52cbb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding matrix...\n",
      "number of null word embeddings: 30\n"
     ]
    }
   ],
   "source": [
    "# Crear la Embedding matrix de las secuencias\n",
    "# en ingles\n",
    "\n",
    "print('preparing embedding matrix...')\n",
    "embed_dim = model_embeddings.N_FEATURES\n",
    "words_not_found = []\n",
    "\n",
    "# word_index provieen del tokenizer\n",
    "\n",
    "nb_words = min(MAX_VOCAB_SIZE, len(word2idx_inputs)) # vocab_size\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "for word, i in word2idx_inputs.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = model_embeddings.get_words_embeddings(word)[0]\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        words_not_found.append(word)\n",
    "\n",
    "print('number of null word embeddings:', np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4q3U_WmEYRdH",
    "outputId": "48a12148-610e-45f0-a8cc-6210a0a1d43a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3851"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FpzJODHBlAtE",
    "outputId": "f97f6d24-4fed-4304-9fbd-501818579ff5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3851, 50)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dimensión de los embeddings de la secuencia en ingles\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vKbhjtIwPgM"
   },
   "source": [
    "### 4 - Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3fm3HCLMPSG-",
    "outputId": "221970f1-7f64-4a10-81c6-e6edf7cf45e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Seq2Seq                                  [1, 18, 5722]             --\n",
       "├─Encoder: 1-1                           [1, 1, 128]               --\n",
       "│    └─Embedding: 2-1                    [1, 16, 50]               (192,550)\n",
       "│    └─LSTM: 2-2                         [1, 16, 128]              92,160\n",
       "├─Decoder: 1-2                           [1, 5722]                 --\n",
       "│    └─Embedding: 2-3                    [1, 1, 50]                286,100\n",
       "│    └─LSTM: 2-4                         [1, 1, 128]               92,160\n",
       "│    └─Linear: 2-5                       [1, 5722]                 738,138\n",
       "│    └─Softmax: 2-6                      [1, 5722]                 --\n",
       "├─Decoder: 1-3                           [1, 5722]                 (recursive)\n",
       "│    └─Embedding: 2-7                    [1, 1, 50]                (recursive)\n",
       "│    └─LSTM: 2-8                         [1, 1, 128]               (recursive)\n",
       "│    └─Linear: 2-9                       [1, 5722]                 (recursive)\n",
       "│    └─Softmax: 2-10                     [1, 5722]                 --\n",
       "├─Decoder: 1-4                           [1, 5722]                 (recursive)\n",
       "│    └─Embedding: 2-11                   [1, 1, 50]                (recursive)\n",
       "│    └─LSTM: 2-12                        [1, 1, 128]               (recursive)\n",
       "│    └─Linear: 2-13                      [1, 5722]                 (recursive)\n",
       "│    └─Softmax: 2-14                     [1, 5722]                 --\n",
       "├─Decoder: 1-5                           [1, 5722]                 (recursive)\n",
       "│    └─Embedding: 2-15                   [1, 1, 50]                (recursive)\n",
       "│    └─LSTM: 2-16                        [1, 1, 128]               (recursive)\n",
       "│    └─Linear: 2-17                      [1, 5722]                 (recursive)\n",
       "│    └─Softmax: 2-18                     [1, 5722]                 --\n",
       "├─Decoder: 1-6                           [1, 5722]                 (recursive)\n",
       "│    └─Embedding: 2-19                   [1, 1, 50]                (recursive)\n",
       "│    └─LSTM: 2-20                        [1, 1, 128]               (recursive)\n",
       "│    └─Linear: 2-21                      [1, 5722]                 (recursive)\n",
       "│    └─Softmax: 2-22                     [1, 5722]                 --\n",
       "├─Decoder: 1-7                           [1, 5722]                 (recursive)\n",
       "│    └─Embedding: 2-23                   [1, 1, 50]                (recursive)\n",
       "│    └─LSTM: 2-24                        [1, 1, 128]               (recursive)\n",
       "│    └─Linear: 2-25                      [1, 5722]                 (recursive)\n",
       "│    └─Softmax: 2-26                     [1, 5722]                 --\n",
       "├─Decoder: 1-8                           [1, 5722]                 (recursive)\n",
       "│    └─Embedding: 2-27                   [1, 1, 50]                (recursive)\n",
       "│    └─LSTM: 2-28                        [1, 1, 128]               (recursive)\n",
       "│    └─Linear: 2-29                      [1, 5722]                 (recursive)\n",
       "│    └─Softmax: 2-30                     [1, 5722]                 --\n",
       "├─Decoder: 1-9                           [1, 5722]                 (recursive)\n",
       "│    └─Embedding: 2-31                   [1, 1, 50]                (recursive)\n",
       "│    └─LSTM: 2-32                        [1, 1, 128]               (recursive)\n",
       "│    └─Linear: 2-33                      [1, 5722]                 (recursive)\n",
       "│    └─Softmax: 2-34                     [1, 5722]                 --\n",
       "├─Decoder: 1-10                          [1, 5722]                 (recursive)\n",
       "│    └─Embedding: 2-35                   [1, 1, 50]                (recursive)\n",
       "│    └─LSTM: 2-36                        [1, 1, 128]               (recursive)\n",
       "│    └─Linear: 2-37                      [1, 5722]                 (recursive)\n",
       "│    └─Softmax: 2-38                     [1, 5722]                 --\n",
       "├─Decoder: 1-11                          [1, 5722]                 (recursive)\n",
       "│    └─Embedding: 2-39                   [1, 1, 50]                (recursive)\n",
       "│    └─LSTM: 2-40                        [1, 1, 128]               (recursive)\n",
       "│    └─Linear: 2-41                      [1, 5722]                 (recursive)\n",
       "│    └─Softmax: 2-42                     [1, 5722]                 --\n",
       "├─Decoder: 1-12                          [1, 5722]                 (recursive)\n",
       "│    └─Embedding: 2-43                   [1, 1, 50]                (recursive)\n",
       "│    └─LSTM: 2-44                        [1, 1, 128]               (recursive)\n",
       "│    └─Linear: 2-45                      [1, 5722]                 (recursive)\n",
       "│    └─Softmax: 2-46                     [1, 5722]                 --\n",
       "├─Decoder: 1-13                          [1, 5722]                 (recursive)\n",
       "│    └─Embedding: 2-47                   [1, 1, 50]                (recursive)\n",
       "│    └─LSTM: 2-48                        [1, 1, 128]               (recursive)\n",
       "│    └─Linear: 2-49                      [1, 5722]                 (recursive)\n",
       "│    └─Softmax: 2-50                     [1, 5722]                 --\n",
       "├─Decoder: 1-14                          [1, 5722]                 (recursive)\n",
       "│    └─Embedding: 2-51                   [1, 1, 50]                (recursive)\n",
       "│    └─LSTM: 2-52                        [1, 1, 128]               (recursive)\n",
       "│    └─Linear: 2-53                      [1, 5722]                 (recursive)\n",
       "│    └─Softmax: 2-54                     [1, 5722]                 --\n",
       "├─Decoder: 1-15                          [1, 5722]                 (recursive)\n",
       "│    └─Embedding: 2-55                   [1, 1, 50]                (recursive)\n",
       "│    └─LSTM: 2-56                        [1, 1, 128]               (recursive)\n",
       "│    └─Linear: 2-57                      [1, 5722]                 (recursive)\n",
       "│    └─Softmax: 2-58                     [1, 5722]                 --\n",
       "├─Decoder: 1-16                          [1, 5722]                 (recursive)\n",
       "│    └─Embedding: 2-59                   [1, 1, 50]                (recursive)\n",
       "│    └─LSTM: 2-60                        [1, 1, 128]               (recursive)\n",
       "│    └─Linear: 2-61                      [1, 5722]                 (recursive)\n",
       "│    └─Softmax: 2-62                     [1, 5722]                 --\n",
       "├─Decoder: 1-17                          [1, 5722]                 (recursive)\n",
       "│    └─Embedding: 2-63                   [1, 1, 50]                (recursive)\n",
       "│    └─LSTM: 2-64                        [1, 1, 128]               (recursive)\n",
       "│    └─Linear: 2-65                      [1, 5722]                 (recursive)\n",
       "│    └─Softmax: 2-66                     [1, 5722]                 --\n",
       "├─Decoder: 1-18                          [1, 5722]                 (recursive)\n",
       "│    └─Embedding: 2-67                   [1, 1, 50]                (recursive)\n",
       "│    └─LSTM: 2-68                        [1, 1, 128]               (recursive)\n",
       "│    └─Linear: 2-69                      [1, 5722]                 (recursive)\n",
       "│    └─Softmax: 2-70                     [1, 5722]                 --\n",
       "├─Decoder: 1-19                          [1, 5722]                 (recursive)\n",
       "│    └─Embedding: 2-71                   [1, 1, 50]                (recursive)\n",
       "│    └─LSTM: 2-72                        [1, 1, 128]               (recursive)\n",
       "│    └─Linear: 2-73                      [1, 5722]                 (recursive)\n",
       "│    └─Softmax: 2-74                     [1, 5722]                 --\n",
       "==========================================================================================\n",
       "Total params: 1,401,108\n",
       "Trainable params: 1,208,558\n",
       "Non-trainable params: 192,550\n",
       "Total mult-adds (Units.MEGABYTES): 21.76\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.87\n",
       "Params size (MB): 5.60\n",
       "Estimated Total Size (MB): 6.48\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # num_embeddings = vocab_size, definido por le Tokenizador\n",
    "        # embedding_dim = 50 --> dimensión de los embeddings utilizados\n",
    "        self.lstm_size = 128\n",
    "        self.num_layers = 1\n",
    "        self.embedding_dim = embed_dim\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.embedding_dim, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = False  # marcar como layer no entrenable (freeze)\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.lstm_size, batch_first=True,\n",
    "                            num_layers=self.num_layers) # LSTM layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        lstm_output, (ht, ct) = self.lstm(out)\n",
    "        return (ht, ct)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, output_dim):\n",
    "        super().__init__()\n",
    "        # num_embeddings = vocab_size, definido por le Tokenizador\n",
    "        # embedding_dim = 50 --> dimensión de los embeddings utilizados\n",
    "        self.lstm_size = 128\n",
    "        self.num_layers = 1\n",
    "        self.embedding_dim = embed_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.lstm_size, batch_first=True,\n",
    "                            num_layers=self.num_layers) # LSTM layer\n",
    "        self.fc1 = nn.Linear(in_features=self.lstm_size, out_features=self.output_dim) # Fully connected layer\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1) # normalize in dim 1\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        out = self.embedding(x)\n",
    "        lstm_output, (ht, ct) = self.lstm(out, prev_state)\n",
    "        out = self.softmax(self.fc1(lstm_output[:,-1,:])) # take last output (last seq)\n",
    "        return out, (ht, ct)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        assert encoder.lstm_size == decoder.lstm_size, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.num_layers == decoder.num_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        batch_size = decoder_input.shape[0]\n",
    "        decoder_input_len = decoder_input.shape[1]\n",
    "        vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # tensor para almacenar la salida\n",
    "        # (batch_size, sentence_len, one_hot_size)\n",
    "        outputs = torch.zeros(batch_size, decoder_input_len, vocab_size)\n",
    "\n",
    "        # ultimo hidden state del encoder, primer estado oculto del decoder\n",
    "        prev_state = self.encoder(encoder_input)\n",
    "\n",
    "        # En la primera iteracion se toma el primer token de target (<sos>)\n",
    "        input = decoder_input[:, 0:1]\n",
    "\n",
    "        for t in range(decoder_input_len):\n",
    "            # t --> token index\n",
    "\n",
    "            # utilizamos método \"teacher forcing\", es decir que durante\n",
    "            # el entrenamiento no realimentamos la salida del decoder\n",
    "            # sino el token correcto que sigue en target\n",
    "            input = decoder_input[:, t:t+1]\n",
    "\n",
    "            # ingresar cada token embedding, uno por uno junto al hidden state\n",
    "            # recibir el output del decoder (softmax)\n",
    "            output, prev_state = self.decoder(input, prev_state)\n",
    "            top1 = output.argmax(1).view(-1, 1)\n",
    "\n",
    "            # Sino se usará \"teacher forcing\" habría que descomentar\n",
    "            # esta linea.\n",
    "            # Hay ejemplos dandos vuelta en donde se utilza un random\n",
    "            # para ver en cada vuelta que técnica se aplica\n",
    "            #input = top1\n",
    "\n",
    "            # guardar cada salida (softmax)\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "        return outputs\n",
    "\n",
    "encoder = Encoder(vocab_size=nb_words)\n",
    "#if cuda: encoder.cuda()\n",
    "# decoder --> vocab_size == output_dim --> porque recibe y devuelve palabras en el mismo vocabulario\n",
    "decoder = Decoder(vocab_size=num_words_output, output_dim=num_words_output)\n",
    "#if cuda: decoder.cuda()\n",
    "\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "if cuda: model.cuda()\n",
    "\n",
    "# Crear el optimizador la una función de error\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Para clasificación multi categórica\n",
    "\n",
    "# summary(model, input_data=(data_set[0:1][0], data_set[0:1][1]))\n",
    "input_data_gpu = data_set[0:1][0].to(device) # <--- ¡Mover a la GPU!\n",
    "target_data_gpu = data_set[0:1][1].to(device) # <--- ¡Mover a la GPU!\n",
    "\n",
    "# Ahora pasamos los tensores ya en la GPU a summary\n",
    "summary(model, input_data=(input_data_gpu, target_data_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDB0KWIegt8s",
    "outputId": "36d180c2-69c4-4d60-edc6-f37a15068f37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 - Train loss 144.855 - Train accuracy 0.606 - Valid Loss 144.899 - Valid accuracy 0.602\n",
      "Epoch: 2/10 - Train loss 144.754 - Train accuracy 0.610 - Valid Loss 144.899 - Valid accuracy 0.602\n",
      "Epoch: 3/10 - Train loss 144.754 - Train accuracy 0.610 - Valid Loss 144.899 - Valid accuracy 0.602\n",
      "Epoch: 4/10 - Train loss 144.754 - Train accuracy 0.610 - Valid Loss 144.899 - Valid accuracy 0.602\n",
      "Epoch: 5/10 - Train loss 144.754 - Train accuracy 0.610 - Valid Loss 144.899 - Valid accuracy 0.602\n",
      "Epoch: 6/10 - Train loss 144.754 - Train accuracy 0.611 - Valid Loss 144.899 - Valid accuracy 0.602\n",
      "Epoch: 7/10 - Train loss 144.753 - Train accuracy 0.611 - Valid Loss 144.899 - Valid accuracy 0.602\n",
      "Epoch: 8/10 - Train loss 144.753 - Train accuracy 0.611 - Valid Loss 144.899 - Valid accuracy 0.602\n",
      "Epoch: 9/10 - Train loss 144.753 - Train accuracy 0.611 - Valid Loss 144.899 - Valid accuracy 0.602\n",
      "Epoch: 10/10 - Train loss 144.538 - Train accuracy 0.623 - Valid Loss 144.177 - Valid accuracy 0.642\n"
     ]
    }
   ],
   "source": [
    "history1 = train(model,\n",
    "                train_loader,\n",
    "                valid_loader,\n",
    "                optimizer,\n",
    "                criterion,\n",
    "                epochs=10\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "pZzm3tx059Zv",
    "outputId": "967468e3-0d38-4eca-8836-ee1b7b43eb84"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_count = range(1, len(history1['accuracy']) + 1)\n",
    "sns.lineplot(x=epoch_count,  y=history1['accuracy'], label='train')\n",
    "sns.lineplot(x=epoch_count,  y=history1['val_accuracy'], label='valid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zbwn0ekDy_s2"
   },
   "source": [
    "### 5 - Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "jnkl3mSpsU_7",
    "outputId": "1dd792f9-df35-493b-c558-08fd009a7032"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nStep 1:\\nA deal is a deal -> Encoder -> enc(h1,c1)\\n\\nenc(h1,c1) + <sos> -> Decoder -> Un + dec(h1,c1)\\n\\nstep 2:\\ndec(h1,c1) + Un -> Decoder -> trato + dec(h2,c2)\\n\\nstep 3:\\ndec(h2,c2) + trato -> Decoder -> es + dec(h3,c3)\\n\\nstep 4:\\ndec(h3,c3) + es -> Decoder -> un + dec(h4,c4)\\n\\nstep 5:\\ndec(h4,c4) + un -> Decoder -> trato + dec(h5,c5)\\n\\nstep 6:\\ndec(h5,c5) + trato. -> Decoder -> <eos> + dec(h6,c6)\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Step 1:\n",
    "A deal is a deal -> Encoder -> enc(h1,c1)\n",
    "\n",
    "enc(h1,c1) + <sos> -> Decoder -> Un + dec(h1,c1)\n",
    "\n",
    "step 2:\n",
    "dec(h1,c1) + Un -> Decoder -> trato + dec(h2,c2)\n",
    "\n",
    "step 3:\n",
    "dec(h2,c2) + trato -> Decoder -> es + dec(h3,c3)\n",
    "\n",
    "step 4:\n",
    "dec(h3,c3) + es -> Decoder -> un + dec(h4,c4)\n",
    "\n",
    "step 5:\n",
    "dec(h4,c4) + un -> Decoder -> trato + dec(h5,c5)\n",
    "\n",
    "step 6:\n",
    "dec(h5,c5) + trato. -> Decoder -> <eos> + dec(h6,c6)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "71XeCtfYmOFx"
   },
   "outputs": [],
   "source": [
    "# Armar lo conversores de indice a palabra:\n",
    "idx2word_input = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_target = {v:k for k, v in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-YOZFX8WkHsQ",
    "outputId": "512afbd6-11bc-476e-8c21-b81a43454ffa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: My mother say hi.\n",
      "Representacion en vector de tokens de ids [15, 225, 134]\n",
      "Padding del vector: [[  0   0   0   0   0   0   0   0   0   0   0   0   0  15 225 134]]\n",
      "Index/token de salida: 6\n",
      "Palabra de salida: tom\n"
     ]
    }
   ],
   "source": [
    "input_test = \"My mother say hi.\"\n",
    "print('Input:', input_test)\n",
    "integer_seq_test = input_tokenizer.texts_to_sequences([input_test])[0]\n",
    "print(\"Representacion en vector de tokens de ids\", integer_seq_test)\n",
    "encoder_sequence_test = pad_sequences([integer_seq_test], maxlen=max_input_len)\n",
    "print(\"Padding del vector:\", encoder_sequence_test)\n",
    "encoder_sequence_test_tensor = torch.from_numpy(encoder_sequence_test.astype(np.int32))\n",
    "\n",
    "# Se obtiene la salida del encoder (el estado oculto para el decoder)\n",
    "prev_state = model.encoder(encoder_sequence_test_tensor.to(device))\n",
    "\n",
    "# Se inicializa la secuencia de entrada al decoder como \"<sos>\"\n",
    "target_seq = np.zeros((1, 1))\n",
    "target_seq[0, 0] = word2idx_outputs['<sos>']\n",
    "target_seq_tensor = torch.from_numpy(target_seq.astype(np.int32))\n",
    "\n",
    "# Se obtiene la primera palabra de la secuencia de salida del decoder\n",
    "output, prev_state = model.decoder(target_seq_tensor.to(device), prev_state)\n",
    "\n",
    "top1 = output.argmax(1).view(-1, 1)\n",
    "idx = int(top1.cpu())\n",
    "print(\"Index/token de salida:\", idx)\n",
    "\n",
    "word = idx2word_target[idx]\n",
    "print(\"Palabra de salida:\", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "MlUyp9M6ua2V"
   },
   "outputs": [],
   "source": [
    "def translate_sentence(input_seq):\n",
    "    # Se transforma la sequencia de entrada a los stados \"h\" y \"c\" de la LSTM\n",
    "    # para enviar la primera vez al decoder\"\n",
    "    prev_state = model.encoder(encoder_sequence_test_tensor.to(device))\n",
    "\n",
    "    # Se inicializa la secuencia de entrada al decoder como \"<sos>\"\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = word2idx_outputs['<sos>']\n",
    "    target_seq_tensor = torch.from_numpy(target_seq.astype(np.int32))\n",
    "\n",
    "    # Se obtiene el indice que finaliza la inferencia\n",
    "    eos = word2idx_outputs['<eos>']\n",
    "\n",
    "    output_sentence = []\n",
    "    for _ in range(max_out_len):\n",
    "        # Predicción del próximo elemento\n",
    "        output, new_prev_state = model.decoder(target_seq_tensor.to(device), prev_state)\n",
    "        top1 = output.argmax(1).view(-1, 1)\n",
    "        idx = int(top1.cpu())\n",
    "\n",
    "        # Si es \"end of sentece <eos>\" se acaba\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        # Transformar ídx a palabra\n",
    "        word = ''\n",
    "        if idx > 0:\n",
    "            word = idx2word_target[idx]\n",
    "            output_sentence.append(word)\n",
    "\n",
    "        # Actualizar los estados dado la ultimo prediccion\n",
    "        prev_state = new_prev_state\n",
    "\n",
    "        # Actualizar secuencia de entrada con la salida (re-alimentacion)\n",
    "        target_seq_tensor = top1\n",
    "\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fd6VJcZb9It_",
    "outputId": "9c535c7a-1fe3-435e-ba58-43683ad23c26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: My mother say hi.\n",
      "Representacion en vector de tokens de ids [15, 225, 134]\n",
      "Padding del vector: [[  0   0   0   0   0   0   0   0   0   0   0   0   0  15 225 134]]\n",
      "Response: tom\n"
     ]
    }
   ],
   "source": [
    "input_test = \"My mother say hi.\"\n",
    "print('Input:', input_test)\n",
    "integer_seq_test = input_tokenizer.texts_to_sequences([input_test])[0]\n",
    "print(\"Representacion en vector de tokens de ids\", integer_seq_test)\n",
    "encoder_sequence_test = pad_sequences([integer_seq_test], maxlen=max_input_len)\n",
    "print(\"Padding del vector:\", encoder_sequence_test)\n",
    "encoder_sequence_test_tensor = torch.from_numpy(encoder_sequence_test.astype(np.int32))\n",
    "\n",
    "translation = translate_sentence(encoder_sequence_test)\n",
    "print('Response:', translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZhGVjLKcunxW",
    "outputId": "1f36a5b0-5af0-4117-cdf5-541879c0ddb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input: Mary wants to buy a dress.\n",
      "Response: tom\n"
     ]
    }
   ],
   "source": [
    "i = np.random.choice(len(input_sentences))\n",
    "input_seq = encoder_input_sequences[i:i+1]\n",
    "encoder_sequence_test_tensor = torch.from_numpy(input_seq.astype(np.int32))\n",
    "translation = translate_sentence(encoder_sequence_test_tensor)\n",
    "print('-')\n",
    "print('Input:', input_sentences[i])\n",
    "print('Response:', translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkOjSJweqdF8"
   },
   "source": [
    "### 6 - Conclusión\n",
    "A primera vista parece que el modelo tendría que funcionar muy bien por el accuracy alcanzado. La realidad es que las respuesta no tienen que ver demasiado con la pregunta/traducción pero la respuesta en si tiene bastante coherencia.\\\n",
    "Para poder mejorar el modelo haría falta poder consumir todo el dataset y todo el vocabulario, pero la cantidad de RAM no es suficiente.\\"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
