{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3yeJGnCYxuF"
   },
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "## Modelo de lenguaje con tokenización por caracteres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iv5PEwGzZA9-"
   },
   "source": [
    "### Consigna\n",
    "- Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje.\n",
    "- Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validación.\n",
    "- Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.\n",
    "- Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determístico y estocástico. En este último caso observar el efecto de la temperatura en la generación de secuencias.\n",
    "\n",
    "\n",
    "### Sugerencias\n",
    "- Durante el entrenamiento, guiarse por el descenso de la perplejidad en los datos de validación para finalizar el entrenamiento. Para ello se provee un callback.\n",
    "- Explorar utilizar SimpleRNN (celda de Elman), LSTM y GRU.\n",
    "- rmsprop es el optimizador recomendado para la buena convergencia. No obstante se pueden explorar otros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Y-QdFbHZYj7C"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import io\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTvXlEKQZdqx"
   },
   "source": [
    "### Datos\n",
    "Utilizaremos como dataset canciones de bandas de habla inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7amy6uUaBLVD"
   },
   "outputs": [],
   "source": [
    "# descargar de textos.info\n",
    "import urllib.request\n",
    "\n",
    "# Para leer y parsear el texto en HTML de wikipedia\n",
    "import bs4 as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6v_ickFwBJTy",
    "outputId": "879e77cd-b1b7-4cb6-8a49-385fe8ff85e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/ezequiel/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Usar Moby Dick como corpus\n",
    "article_text = gutenberg.raw('melville-moby_dick.txt')\n",
    "article_text = article_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "WBE0sSYuB-E6",
    "outputId": "be0fd28c-00c4-45a2-d2a7-d0d83de5373c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[moby dick by herman melville 1851]\\r\\n\\r\\n\\r\\netymology.\\r\\n\\r\\n(supplied by a late consumptive usher to a grammar school)\\r\\n\\r\\nthe pale usher--threadbare in coat, heart, body, and brain; i see him\\r\\nnow.  he was ever dusting his old lexicons and grammars, with a queer\\r\\nhandkerchief, mockingly embellished with all the gay flags of all the\\r\\nknown nations of the world.  he loved to dust his old grammars; it\\r\\nsomehow mildly reminded him of his mortality.\\r\\n\\r\\n\"while you take in hand to school others, and to teach them by what\\r\\nname a whale-fish is to be called in our tongue leaving out, through\\r\\nignorance, the letter h, which almost alone maketh the signification\\r\\nof the word, you deliver that which is not true.\" --hackluyt\\r\\n\\r\\n\"whale. ... sw. and dan. hval.  this animal is named from roundness\\r\\nor rolling; for in dan. hvalt is arched or vaulted.\" --webster\\'s\\r\\ndictionary\\r\\n\\r\\n\"whale. ... it is more immediately from the dut. and ger. wallen;\\r\\na.s. walw-ian, to roll, to wallow.\" --richardson\\'s dictionary\\r\\n\\r'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# en article text se encuentra el texto de todo el libro\n",
    "article_text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cP1JdiOIKQWi"
   },
   "source": [
    "### Elegir el tamaño del contexto\n",
    "\n",
    "En este caso, como el modelo de lenguaje es por caracteres, todo un gran corpus\n",
    "de texto puede ser considerado un documento en sí mismo y el tamaño de contexto\n",
    "puede ser elegido con más libertad en comparación a un modelo de lenguaje tokenizado por palabras y dividido en documentos más acotados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wumBNwdjJM3j"
   },
   "outputs": [],
   "source": [
    "# seleccionamos el tamaño de contexto\n",
    "max_context_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "m5FeTaGvbDbw"
   },
   "outputs": [],
   "source": [
    "# Usaremos las utilidades de procesamiento de textos y secuencias de Keras\n",
    "from tensorflow.keras.utils import pad_sequences # se utilizará para padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "573Cg5n7VhWw"
   },
   "outputs": [],
   "source": [
    "# en este caso el vocabulario es el conjunto único de caracteres que existe en todo el texto\n",
    "chars_vocab = set(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VwTK6xgLJd8q",
    "outputId": "23ad1202-ac74-47a2-8985-99bcfba3f0c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# la longitud de vocabulario de caracteres es:\n",
    "len(chars_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2W0AeQjXV1Ou"
   },
   "outputs": [],
   "source": [
    "# Construimos los dicionarios que asignan índices a caracteres y viceversa.\n",
    "# El diccionario `char2idx` servirá como tokenizador.\n",
    "char2idx = {k: v for v,k in enumerate(chars_vocab)}\n",
    "idx2char = {v: k for k,v in char2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oIUjVU0LB0r"
   },
   "source": [
    "###  Tokenizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "h07G3srdJppo"
   },
   "outputs": [],
   "source": [
    "# tokenizamos el texto completo\n",
    "tokenized_text = [char2idx[ch] for ch in article_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PwGVSKOiJ5bj",
    "outputId": "ff5cd9fb-3081-446e-e29f-7592a2f5c219"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42,\n",
       " 0,\n",
       " 10,\n",
       " 3,\n",
       " 37,\n",
       " 55,\n",
       " 18,\n",
       " 1,\n",
       " 53,\n",
       " 8,\n",
       " 55,\n",
       " 3,\n",
       " 37,\n",
       " 55,\n",
       " 43,\n",
       " 15,\n",
       " 4,\n",
       " 0,\n",
       " 22,\n",
       " 20,\n",
       " 55,\n",
       " 0,\n",
       " 15,\n",
       " 39,\n",
       " 9,\n",
       " 1,\n",
       " 39,\n",
       " 39,\n",
       " 15,\n",
       " 55,\n",
       " 38,\n",
       " 40,\n",
       " 13,\n",
       " 38,\n",
       " 48,\n",
       " 26,\n",
       " 44,\n",
       " 26,\n",
       " 44,\n",
       " 26,\n",
       " 44,\n",
       " 15,\n",
       " 14,\n",
       " 37,\n",
       " 0,\n",
       " 10,\n",
       " 39,\n",
       " 10,\n",
       " 27,\n",
       " 37,\n",
       " 52,\n",
       " 26,\n",
       " 44,\n",
       " 26,\n",
       " 44,\n",
       " 16,\n",
       " 41,\n",
       " 49,\n",
       " 36,\n",
       " 36,\n",
       " 39,\n",
       " 1,\n",
       " 15,\n",
       " 18,\n",
       " 55,\n",
       " 3,\n",
       " 37,\n",
       " 55,\n",
       " 22,\n",
       " 55,\n",
       " 39,\n",
       " 22,\n",
       " 14,\n",
       " 15,\n",
       " 55,\n",
       " 53,\n",
       " 10,\n",
       " 20,\n",
       " 41,\n",
       " 49,\n",
       " 0,\n",
       " 36,\n",
       " 14,\n",
       " 1,\n",
       " 9,\n",
       " 15,\n",
       " 55,\n",
       " 49,\n",
       " 41,\n",
       " 43,\n",
       " 15,\n",
       " 4,\n",
       " 55,\n",
       " 14,\n",
       " 10,\n",
       " 55,\n",
       " 22,\n",
       " 55,\n",
       " 27,\n",
       " 4,\n",
       " 22,\n",
       " 0,\n",
       " 0,\n",
       " 22,\n",
       " 4,\n",
       " 55,\n",
       " 41,\n",
       " 53,\n",
       " 43,\n",
       " 10,\n",
       " 10,\n",
       " 39,\n",
       " 28,\n",
       " 26,\n",
       " 44,\n",
       " 26,\n",
       " 44,\n",
       " 14,\n",
       " 43,\n",
       " 15,\n",
       " 55,\n",
       " 36,\n",
       " 22,\n",
       " 39,\n",
       " 15,\n",
       " 55,\n",
       " 49,\n",
       " 41,\n",
       " 43,\n",
       " 15,\n",
       " 4,\n",
       " 35,\n",
       " 35,\n",
       " 14,\n",
       " 43,\n",
       " 4,\n",
       " 15,\n",
       " 22,\n",
       " 18,\n",
       " 3,\n",
       " 22,\n",
       " 4,\n",
       " 15,\n",
       " 55,\n",
       " 1,\n",
       " 20,\n",
       " 55,\n",
       " 53,\n",
       " 10,\n",
       " 22,\n",
       " 14,\n",
       " 29,\n",
       " 55,\n",
       " 43,\n",
       " 15,\n",
       " 22,\n",
       " 4,\n",
       " 14,\n",
       " 29,\n",
       " 55,\n",
       " 3,\n",
       " 10,\n",
       " 18,\n",
       " 37,\n",
       " 29,\n",
       " 55,\n",
       " 22,\n",
       " 20,\n",
       " 18,\n",
       " 55,\n",
       " 3,\n",
       " 4,\n",
       " 22,\n",
       " 1,\n",
       " 20,\n",
       " 25,\n",
       " 55,\n",
       " 1,\n",
       " 55,\n",
       " 41,\n",
       " 15,\n",
       " 15,\n",
       " 55,\n",
       " 43,\n",
       " 1,\n",
       " 0,\n",
       " 26,\n",
       " 44,\n",
       " 20,\n",
       " 10,\n",
       " 21,\n",
       " 52,\n",
       " 55,\n",
       " 55,\n",
       " 43,\n",
       " 15,\n",
       " 55,\n",
       " 21,\n",
       " 22,\n",
       " 41,\n",
       " 55,\n",
       " 15,\n",
       " 9,\n",
       " 15,\n",
       " 4,\n",
       " 55,\n",
       " 18,\n",
       " 49,\n",
       " 41,\n",
       " 14,\n",
       " 1,\n",
       " 20,\n",
       " 27,\n",
       " 55,\n",
       " 43,\n",
       " 1,\n",
       " 41,\n",
       " 55,\n",
       " 10,\n",
       " 39,\n",
       " 18,\n",
       " 55,\n",
       " 39,\n",
       " 15,\n",
       " 11,\n",
       " 1,\n",
       " 53,\n",
       " 10,\n",
       " 20,\n",
       " 41,\n",
       " 55,\n",
       " 22,\n",
       " 20,\n",
       " 18,\n",
       " 55,\n",
       " 27,\n",
       " 4,\n",
       " 22,\n",
       " 0,\n",
       " 0,\n",
       " 22,\n",
       " 4,\n",
       " 41,\n",
       " 29,\n",
       " 55,\n",
       " 21,\n",
       " 1,\n",
       " 14,\n",
       " 43,\n",
       " 55,\n",
       " 22,\n",
       " 55,\n",
       " 17,\n",
       " 49,\n",
       " 15,\n",
       " 15,\n",
       " 4,\n",
       " 26,\n",
       " 44,\n",
       " 43,\n",
       " 22,\n",
       " 20,\n",
       " 18,\n",
       " 8,\n",
       " 15,\n",
       " 4,\n",
       " 53,\n",
       " 43,\n",
       " 1,\n",
       " 15,\n",
       " 19,\n",
       " 29,\n",
       " 55,\n",
       " 0,\n",
       " 10,\n",
       " 53,\n",
       " 8,\n",
       " 1,\n",
       " 20,\n",
       " 27,\n",
       " 39,\n",
       " 37,\n",
       " 55,\n",
       " 15,\n",
       " 0,\n",
       " 3,\n",
       " 15,\n",
       " 39,\n",
       " 39,\n",
       " 1,\n",
       " 41,\n",
       " 43,\n",
       " 15,\n",
       " 18,\n",
       " 55,\n",
       " 21,\n",
       " 1,\n",
       " 14,\n",
       " 43,\n",
       " 55,\n",
       " 22,\n",
       " 39,\n",
       " 39,\n",
       " 55,\n",
       " 14,\n",
       " 43,\n",
       " 15,\n",
       " 55,\n",
       " 27,\n",
       " 22,\n",
       " 37,\n",
       " 55,\n",
       " 19,\n",
       " 39,\n",
       " 22,\n",
       " 27,\n",
       " 41,\n",
       " 55,\n",
       " 10,\n",
       " 19,\n",
       " 55,\n",
       " 22,\n",
       " 39,\n",
       " 39,\n",
       " 55,\n",
       " 14,\n",
       " 43,\n",
       " 15,\n",
       " 26,\n",
       " 44,\n",
       " 8,\n",
       " 20,\n",
       " 10,\n",
       " 21,\n",
       " 20,\n",
       " 55,\n",
       " 20,\n",
       " 22,\n",
       " 14,\n",
       " 1,\n",
       " 10,\n",
       " 20,\n",
       " 41,\n",
       " 55,\n",
       " 10,\n",
       " 19,\n",
       " 55,\n",
       " 14,\n",
       " 43,\n",
       " 15,\n",
       " 55,\n",
       " 21,\n",
       " 10,\n",
       " 4,\n",
       " 39,\n",
       " 18,\n",
       " 52,\n",
       " 55,\n",
       " 55,\n",
       " 43,\n",
       " 15,\n",
       " 55,\n",
       " 39,\n",
       " 10,\n",
       " 9,\n",
       " 15,\n",
       " 18,\n",
       " 55,\n",
       " 14,\n",
       " 10,\n",
       " 55,\n",
       " 18,\n",
       " 49,\n",
       " 41,\n",
       " 14,\n",
       " 55,\n",
       " 43,\n",
       " 1,\n",
       " 41,\n",
       " 55,\n",
       " 10,\n",
       " 39,\n",
       " 18,\n",
       " 55,\n",
       " 27,\n",
       " 4,\n",
       " 22,\n",
       " 0,\n",
       " 0,\n",
       " 22,\n",
       " 4,\n",
       " 41,\n",
       " 25,\n",
       " 55,\n",
       " 1,\n",
       " 14,\n",
       " 26,\n",
       " 44,\n",
       " 41,\n",
       " 10,\n",
       " 0,\n",
       " 15,\n",
       " 43,\n",
       " 10,\n",
       " 21,\n",
       " 55,\n",
       " 0,\n",
       " 1,\n",
       " 39,\n",
       " 18,\n",
       " 39,\n",
       " 37,\n",
       " 55,\n",
       " 4,\n",
       " 15,\n",
       " 0,\n",
       " 1,\n",
       " 20,\n",
       " 18,\n",
       " 15,\n",
       " 18,\n",
       " 55,\n",
       " 43,\n",
       " 1,\n",
       " 0,\n",
       " 55,\n",
       " 10,\n",
       " 19,\n",
       " 55,\n",
       " 43,\n",
       " 1,\n",
       " 41,\n",
       " 55,\n",
       " 0,\n",
       " 10,\n",
       " 4,\n",
       " 14,\n",
       " 22,\n",
       " 39,\n",
       " 1,\n",
       " 14,\n",
       " 37,\n",
       " 52,\n",
       " 26,\n",
       " 44,\n",
       " 26,\n",
       " 44,\n",
       " 24,\n",
       " 21,\n",
       " 43,\n",
       " 1,\n",
       " 39,\n",
       " 15,\n",
       " 55,\n",
       " 37,\n",
       " 10,\n",
       " 49,\n",
       " 55,\n",
       " 14,\n",
       " 22,\n",
       " 8,\n",
       " 15,\n",
       " 55,\n",
       " 1,\n",
       " 20,\n",
       " 55,\n",
       " 43,\n",
       " 22,\n",
       " 20,\n",
       " 18,\n",
       " 55,\n",
       " 14,\n",
       " 10,\n",
       " 55,\n",
       " 41,\n",
       " 53,\n",
       " 43,\n",
       " 10,\n",
       " 10,\n",
       " 39,\n",
       " 55,\n",
       " 10,\n",
       " 14,\n",
       " 43,\n",
       " 15,\n",
       " 4,\n",
       " 41,\n",
       " 29,\n",
       " 55,\n",
       " 22,\n",
       " 20,\n",
       " 18,\n",
       " 55,\n",
       " 14,\n",
       " 10,\n",
       " 55,\n",
       " 14,\n",
       " 15,\n",
       " 22,\n",
       " 53,\n",
       " 43,\n",
       " 55,\n",
       " 14,\n",
       " 43,\n",
       " 15,\n",
       " 0,\n",
       " 55,\n",
       " 3,\n",
       " 37,\n",
       " 55,\n",
       " 21,\n",
       " 43,\n",
       " 22,\n",
       " 14,\n",
       " 26,\n",
       " 44,\n",
       " 20,\n",
       " 22,\n",
       " 0,\n",
       " 15,\n",
       " 55,\n",
       " 22,\n",
       " 55,\n",
       " 21,\n",
       " 43,\n",
       " 22,\n",
       " 39,\n",
       " 15,\n",
       " 35,\n",
       " 19,\n",
       " 1,\n",
       " 41,\n",
       " 43,\n",
       " 55,\n",
       " 1,\n",
       " 41,\n",
       " 55,\n",
       " 14,\n",
       " 10,\n",
       " 55,\n",
       " 3,\n",
       " 15,\n",
       " 55,\n",
       " 53,\n",
       " 22,\n",
       " 39,\n",
       " 39,\n",
       " 15,\n",
       " 18,\n",
       " 55,\n",
       " 1,\n",
       " 20,\n",
       " 55,\n",
       " 10,\n",
       " 49,\n",
       " 4,\n",
       " 55,\n",
       " 14,\n",
       " 10,\n",
       " 20,\n",
       " 27,\n",
       " 49,\n",
       " 15,\n",
       " 55,\n",
       " 39,\n",
       " 15,\n",
       " 22,\n",
       " 9,\n",
       " 1,\n",
       " 20,\n",
       " 27,\n",
       " 55,\n",
       " 10,\n",
       " 49,\n",
       " 14,\n",
       " 29,\n",
       " 55,\n",
       " 14,\n",
       " 43,\n",
       " 4,\n",
       " 10,\n",
       " 49,\n",
       " 27,\n",
       " 43,\n",
       " 26,\n",
       " 44,\n",
       " 1,\n",
       " 27,\n",
       " 20,\n",
       " 10,\n",
       " 4,\n",
       " 22,\n",
       " 20,\n",
       " 53,\n",
       " 15,\n",
       " 29,\n",
       " 55,\n",
       " 14,\n",
       " 43,\n",
       " 15,\n",
       " 55,\n",
       " 39,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 4,\n",
       " 55,\n",
       " 43,\n",
       " 29,\n",
       " 55,\n",
       " 21,\n",
       " 43,\n",
       " 1,\n",
       " 53,\n",
       " 43,\n",
       " 55,\n",
       " 22,\n",
       " 39,\n",
       " 0,\n",
       " 10,\n",
       " 41,\n",
       " 14,\n",
       " 55,\n",
       " 22,\n",
       " 39,\n",
       " 10,\n",
       " 20,\n",
       " 15,\n",
       " 55,\n",
       " 0,\n",
       " 22,\n",
       " 8,\n",
       " 15,\n",
       " 14,\n",
       " 43,\n",
       " 55,\n",
       " 14,\n",
       " 43,\n",
       " 15,\n",
       " 55,\n",
       " 41,\n",
       " 1,\n",
       " 27,\n",
       " 20,\n",
       " 1,\n",
       " 19,\n",
       " 1,\n",
       " 53,\n",
       " 22,\n",
       " 14,\n",
       " 1,\n",
       " 10,\n",
       " 20,\n",
       " 26,\n",
       " 44,\n",
       " 10,\n",
       " 19,\n",
       " 55,\n",
       " 14,\n",
       " 43,\n",
       " 15,\n",
       " 55,\n",
       " 21,\n",
       " 10,\n",
       " 4,\n",
       " 18,\n",
       " 29,\n",
       " 55,\n",
       " 37,\n",
       " 10,\n",
       " 49,\n",
       " 55,\n",
       " 18,\n",
       " 15,\n",
       " 39,\n",
       " 1,\n",
       " 9,\n",
       " 15,\n",
       " 4,\n",
       " 55,\n",
       " 14,\n",
       " 43,\n",
       " 22,\n",
       " 14,\n",
       " 55,\n",
       " 21,\n",
       " 43,\n",
       " 1,\n",
       " 53,\n",
       " 43,\n",
       " 55,\n",
       " 1,\n",
       " 41,\n",
       " 55,\n",
       " 20,\n",
       " 10,\n",
       " 14,\n",
       " 55,\n",
       " 14,\n",
       " 4,\n",
       " 49,\n",
       " 15,\n",
       " 52,\n",
       " 24,\n",
       " 55,\n",
       " 35,\n",
       " 35,\n",
       " 43,\n",
       " 22,\n",
       " 53,\n",
       " 8,\n",
       " 39,\n",
       " 49,\n",
       " 37,\n",
       " 14,\n",
       " 26,\n",
       " 44,\n",
       " 26,\n",
       " 44,\n",
       " 24,\n",
       " 21,\n",
       " 43,\n",
       " 22,\n",
       " 39,\n",
       " 15,\n",
       " 52,\n",
       " 55,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 55,\n",
       " 41,\n",
       " 21,\n",
       " 52,\n",
       " 55,\n",
       " 22,\n",
       " 20,\n",
       " 18,\n",
       " 55,\n",
       " 18,\n",
       " 22,\n",
       " 20,\n",
       " 52,\n",
       " 55,\n",
       " 43,\n",
       " 9,\n",
       " 22,\n",
       " 39,\n",
       " 52,\n",
       " 55,\n",
       " 55,\n",
       " 14,\n",
       " 43,\n",
       " 1,\n",
       " 41,\n",
       " 55,\n",
       " 22,\n",
       " 20,\n",
       " 1,\n",
       " 0,\n",
       " 22,\n",
       " 39,\n",
       " 55,\n",
       " 1,\n",
       " 41,\n",
       " 55,\n",
       " 20,\n",
       " 22,\n",
       " 0,\n",
       " 15,\n",
       " 18,\n",
       " 55,\n",
       " 19,\n",
       " 4,\n",
       " 10,\n",
       " 0,\n",
       " 55,\n",
       " 4,\n",
       " 10,\n",
       " 49,\n",
       " 20,\n",
       " 18,\n",
       " 20,\n",
       " 15,\n",
       " 41,\n",
       " 41,\n",
       " 26,\n",
       " 44,\n",
       " 10,\n",
       " 4,\n",
       " 55,\n",
       " 4,\n",
       " 10,\n",
       " 39,\n",
       " 39,\n",
       " 1,\n",
       " 20,\n",
       " 27,\n",
       " 25,\n",
       " 55,\n",
       " 19,\n",
       " 10,\n",
       " 4,\n",
       " 55,\n",
       " 1,\n",
       " 20,\n",
       " 55,\n",
       " 18,\n",
       " 22,\n",
       " 20,\n",
       " 52,\n",
       " 55,\n",
       " 43,\n",
       " 9,\n",
       " 22,\n",
       " 39,\n",
       " 14,\n",
       " 55,\n",
       " 1,\n",
       " 41,\n",
       " 55,\n",
       " 22,\n",
       " 4,\n",
       " 53,\n",
       " 43,\n",
       " 15,\n",
       " 18,\n",
       " 55,\n",
       " 10,\n",
       " 4,\n",
       " 55,\n",
       " 9,\n",
       " 22,\n",
       " 49,\n",
       " 39,\n",
       " 14,\n",
       " 15,\n",
       " 18,\n",
       " 52,\n",
       " 24,\n",
       " 55,\n",
       " 35,\n",
       " 35,\n",
       " 21,\n",
       " 15,\n",
       " 3,\n",
       " 41,\n",
       " 14,\n",
       " 15,\n",
       " 4,\n",
       " 7,\n",
       " 41,\n",
       " 26,\n",
       " 44,\n",
       " 18,\n",
       " 1,\n",
       " 53,\n",
       " 14,\n",
       " 1,\n",
       " 10,\n",
       " 20,\n",
       " 22,\n",
       " 4,\n",
       " 37,\n",
       " 26,\n",
       " 44,\n",
       " 26,\n",
       " 44,\n",
       " 24,\n",
       " 21,\n",
       " 43,\n",
       " 22,\n",
       " 39,\n",
       " 15,\n",
       " 52,\n",
       " 55,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 55,\n",
       " 1,\n",
       " 14,\n",
       " 55,\n",
       " 1,\n",
       " 41,\n",
       " 55,\n",
       " 0,\n",
       " 10,\n",
       " 4,\n",
       " 15,\n",
       " 55,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 15,\n",
       " 18,\n",
       " 1,\n",
       " 22,\n",
       " 14,\n",
       " 15,\n",
       " 39,\n",
       " 37,\n",
       " 55,\n",
       " 19,\n",
       " 4,\n",
       " 10,\n",
       " 0,\n",
       " 55,\n",
       " 14,\n",
       " 43,\n",
       " 15,\n",
       " 55,\n",
       " 18,\n",
       " 49,\n",
       " 14,\n",
       " 52,\n",
       " 55,\n",
       " 22,\n",
       " 20,\n",
       " 18,\n",
       " 55,\n",
       " 27,\n",
       " 15,\n",
       " 4,\n",
       " 52,\n",
       " 55,\n",
       " 21,\n",
       " 22,\n",
       " 39,\n",
       " 39,\n",
       " 15,\n",
       " 20,\n",
       " 25,\n",
       " 26,\n",
       " 44,\n",
       " 22,\n",
       " 52,\n",
       " 41,\n",
       " 52,\n",
       " 55,\n",
       " 21,\n",
       " 22,\n",
       " 39,\n",
       " 21,\n",
       " 35,\n",
       " 1,\n",
       " 22,\n",
       " 20,\n",
       " 29,\n",
       " 55,\n",
       " 14,\n",
       " 10,\n",
       " 55,\n",
       " 4,\n",
       " 10,\n",
       " 39,\n",
       " 39,\n",
       " 29,\n",
       " 55,\n",
       " 14,\n",
       " 10,\n",
       " 55,\n",
       " 21,\n",
       " 22,\n",
       " 39,\n",
       " 39,\n",
       " 10,\n",
       " 21,\n",
       " 52,\n",
       " 24,\n",
       " 55,\n",
       " 35,\n",
       " 35,\n",
       " 4,\n",
       " 1,\n",
       " 53,\n",
       " 43,\n",
       " 22,\n",
       " 4,\n",
       " 18,\n",
       " 41,\n",
       " 10,\n",
       " 20,\n",
       " 7,\n",
       " 41,\n",
       " 55,\n",
       " 18,\n",
       " 1,\n",
       " 53,\n",
       " 14,\n",
       " 1,\n",
       " 10,\n",
       " 20,\n",
       " 22,\n",
       " 4,\n",
       " 37,\n",
       " 26,\n",
       " 44,\n",
       " 26]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfpYcaypKcI9"
   },
   "source": [
    "### Organizando y estructurando el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WSSmg9jtKP0T"
   },
   "outputs": [],
   "source": [
    "# separaremos el dataset entre entrenamiento y validación.\n",
    "# `p_val` será la proporción del corpus que se reservará para validación\n",
    "# `num_val` es la cantidad de secuencias de tamaño `max_context_size` que se usará en validación\n",
    "p_val = 0.1\n",
    "num_val = int(np.ceil(len(tokenized_text)*p_val/max_context_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "b7dCpGrdKll0"
   },
   "outputs": [],
   "source": [
    "# separamos la porción de texto utilizada en entrenamiento de la de validación.\n",
    "train_text = tokenized_text[:-num_val*max_context_size]\n",
    "val_text = tokenized_text[-num_val*max_context_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NmxQdxl8LRCg"
   },
   "outputs": [],
   "source": [
    "tokenized_sentences_val = [val_text[init*max_context_size:init*(max_context_size+1)] for init in range(num_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_gyFT9koLqDm"
   },
   "outputs": [],
   "source": [
    "tokenized_sentences_train = [train_text[init:init+max_context_size] for init in range(len(train_text)-max_context_size+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "oVNqmmLRodT0"
   },
   "outputs": [],
   "source": [
    "X = np.array(tokenized_sentences_train[:-1])\n",
    "y = np.array(tokenized_sentences_train[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vken7O4ETsAJ"
   },
   "source": [
    "Nótese que estamos estructurando el problema de aprendizaje como *many-to-many*:\n",
    "\n",
    "Entrada: secuencia de tokens [$x_0$, $x_1$, ..., $x_N$]\n",
    "\n",
    "Target: secuencia de tokens [$x_1$, $x_2$, ..., $x_{N+1}$]\n",
    "\n",
    "De manera que la red tiene que aprender que su salida deben ser los tokens desplazados en una posición y un nuevo token predicho (el N+1).\n",
    "\n",
    "La ventaja de estructurar el aprendizaje de esta manera es que para cada token de target se propaga una señal de gradiente por el grafo de cómputo recurrente, que es mejor que estructurar el problema como *many-to-one* en donde sólo una señal de gradiente se propaga."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3iPTx-UJl6r"
   },
   "source": [
    "En este punto tenemos en la variable `tokenized_sentences` los versos tokenizados. Vamos a quedarnos con un conjunto de validación que utilizaremos para medir la calidad de la generación de secuencias con la métrica de Perplejidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KFAyA4zCWE-5",
    "outputId": "1e298dde-54ef-4ae2-907a-1980d7d5639d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1118590, 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qcKRl70HFTzG",
    "outputId": "65f7d4d8-a5c4-457c-b46b-b8cf5b07bc72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([42,  0, 10,  3, 37, 55, 18,  1, 53,  8])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TVpLCKSZFXZO",
    "outputId": "6e078fdf-0fe5-4cc3-8f75-a70394466168"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 10,  3, 37, 55, 18,  1, 53,  8, 55])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "wOFCR-KqbW1N"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(chars_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnnjdAQ5UAEJ"
   },
   "source": [
    "# Definiendo el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "rkMCZvmhrQz4"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, TimeDistributed, CategoryEncoding, SimpleRNN, Dense\n",
    "from keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgz7VKwTUbj6"
   },
   "source": [
    "El modelo que se propone como ejemplo consume los índices de los tokens y los transforma en vectores OHE (en este caso no entrenamos una capa de embedding para caracteres). Esa transformación se logra combinando las capas `CategoryEncoding` que transforma a índices a vectores OHE y `TimeDistributed` que aplica la capa a lo largo de la dimensión \"temporal\" de la secuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zd2OkfQYs2Q7",
    "outputId": "152e7ed6-7f91-466a-a3da-dc7b4d6623cb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import TimeDistributed, LSTM, Dense\n",
    "from tensorflow.keras.layers import CategoryEncoding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(CategoryEncoding(num_tokens=vocab_size, output_mode=\"one_hot\"), input_shape=(None,1)))\n",
    "model.add(LSTM(256, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmJWNyxQwfCE"
   },
   "source": [
    "\n",
    "### Definir el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWK3z85sQfUe"
   },
   "source": [
    "Dado que por el momento no hay implementaciones adecuadas de la perplejidad que puedan operar en tiempo de entrenamiento, armaremos un Callback *ad-hoc* que la calcule en cada epoch.\n",
    "\n",
    "**Nota**: un Callback es una rutina gatillada por algún evento, son muy útiles para relevar datos en diferentes momentos del desarrollo del modelo. En este caso queremos hacer un cálculo cada vez que termina una epoch de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "zUHX3r5JD-MG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "class PplCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, val_data_raw, history_ppl, patience=5, sample_size=5000): # Añadimos sample_size\n",
    "        self.history_ppl = history_ppl\n",
    "        self.min_score = np.inf\n",
    "        self.patience_counter = 0\n",
    "        self.patience = patience\n",
    "        self.model_saved_path = \"my_model.keras\" # Ruta para guardar el modelo\n",
    "\n",
    "        # Reducir el tamaño de los datos de validación si es necesario\n",
    "        # Tomamos un sample aleatorio si val_data_raw es muy grande\n",
    "        if len(val_data_raw) > sample_size:\n",
    "            # Convertimos a np.array para indexación aleatoria, si no lo es ya\n",
    "            val_data_raw = np.array(val_data_raw, dtype=object)\n",
    "            indices = np.random.choice(len(val_data_raw), sample_size, replace=False)\n",
    "            val_data = val_data_raw[indices]\n",
    "        else:\n",
    "            val_data = val_data_raw\n",
    "\n",
    "        self.target = []\n",
    "        self.padded_inputs = [] # Renombrado para mayor claridad\n",
    "        self.info = [] # Lista de (start, end) para reconstruir secuencias\n",
    "        current_idx = 0 # Usamos un índice continuo para las predicciones\n",
    "\n",
    "        for seq in val_data:\n",
    "            len_seq = len(seq)\n",
    "            if len_seq > 1: # Necesitamos al menos 2 tokens para tener un input y un target\n",
    "                # Inputs: [seq[:1], seq[:2], ..., seq[:len_seq-1]]\n",
    "                # Targets: [seq[1], seq[2], ..., seq[len_seq-1]]\n",
    "                subseq_inputs = [seq[:i] for i in range(1, len_seq)]\n",
    "                self.target.extend([seq[i] for i in range(1, len_seq)])\n",
    "\n",
    "                # Asegurarse de que max_context_size esté definido globalmente o pasarlo\n",
    "                # Aquí asumo que max_context_size está disponible o es un atributo de la clase\n",
    "                # Si no, deberías pasarlo como parámetro al callback\n",
    "                padded_subseq = pad_sequences(subseq_inputs, maxlen=max_context_size, padding='pre')\n",
    "                self.padded_inputs.append(padded_subseq)\n",
    "\n",
    "                # Guardamos el rango de índices para esta secuencia en las predicciones\n",
    "                self.info.append((current_idx, current_idx + len(padded_subseq)))\n",
    "                current_idx += len(padded_subseq)\n",
    "\n",
    "        # Concatenar todos los arrays acolchados en uno solo\n",
    "        if self.padded_inputs: # Asegurarse de que no esté vacío antes de vstack\n",
    "            self.padded_inputs = np.vstack(self.padded_inputs)\n",
    "        else:\n",
    "            self.padded_inputs = np.array([]) # O un array vacío de forma adecuada\n",
    "            print(\"Advertencia: No se generaron datos para el cálculo de perplejidad.\")\n",
    "\n",
    "        self.target = np.array(self.target) # Convertir a NumPy array\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.padded_inputs.size == 0:\n",
    "            print(\"\\n No hay datos de validación para calcular la perplejidad en esta época. \\n\")\n",
    "            return\n",
    "\n",
    "        # Realizar una única predicción para todos los inputs acolchados\n",
    "        # Esto debería ser mucho más rápido ya que TensorFlow lo optimiza para la GPU\n",
    "        predictions_all = self.model.predict(self.padded_inputs, verbose=0)\n",
    "\n",
    "        # Extraer las probabilidades de los tokens objetivo de forma vectorizada\n",
    "        # predictions_all.shape: (num_total_subsequences, max_context_size, vocab_size)\n",
    "        # Queremos la predicción del último token de cada secuencia (-1) y la probabilidad del target\n",
    "        # Usamos np.arange para obtener los índices de las filas correspondientes\n",
    "        # y self.target para los índices de las columnas (vocab_size)\n",
    "        # Primero, asegurémonos de que self.target sea un NumPy array y los índices sean enteros.\n",
    "        # Convertimos self.target a int si no lo es ya\n",
    "        target_indices = self.target.astype(int)\n",
    "\n",
    "        # probs_of_targets contendrá la probabilidad que el modelo asignó al token \"correcto\"\n",
    "        # para cada una de las predicciones hechas.\n",
    "        # Esta es la parte más crítica para vectorizar.\n",
    "        # Necesitamos las probabilidades de `predictions_all[idx_seq, -1, target_token_idx]`\n",
    "        # para todos los idx_seq y sus respectivos target_token_idx.\n",
    "        # Esto se puede hacer con indexación avanzada de NumPy:\n",
    "        predicted_probabilities = predictions_all[np.arange(len(predictions_all)), -1, target_indices]\n",
    "\n",
    "        # Reemplazar ceros por un número muy pequeño para evitar log(0)\n",
    "        predicted_probabilities[predicted_probabilities == 0] = np.finfo(float).eps\n",
    "\n",
    "        # Calcular el logaritmo de las probabilidades\n",
    "        log_probs = np.log(predicted_probabilities)\n",
    "\n",
    "        scores = []\n",
    "        # Acumular los log_probs por secuencia original\n",
    "        for start, end in self.info:\n",
    "            # Sumamos los log_probs de los tokens dentro de cada secuencia original\n",
    "            sum_log_probs_seq = np.sum(log_probs[start:end])\n",
    "            # Calculamos la longitud de la secuencia de targets\n",
    "            len_targets_seq = end - start\n",
    "            if len_targets_seq > 0: # Evitar división por cero\n",
    "                scores.append(np.exp(-sum_log_probs_seq / len_targets_seq))\n",
    "            else:\n",
    "                scores.append(0.0) # O algún valor apropiado para secuencias vacías de target\n",
    "\n",
    "        current_score = np.mean(scores)\n",
    "        self.history_ppl.append(current_score) # Asumo que history_ppl es la misma lista de fuera\n",
    "        print(f'\\n Perplejidad promedio: {current_score:.4f} \\n') # Formatear para mejor lectura\n",
    "\n",
    "        # Chequeamos si tenemos que detener el entrenamiento\n",
    "        if current_score < self.min_score:\n",
    "            self.min_score = current_score\n",
    "            self.model.save(self.model_saved_path)\n",
    "            print(f\"Modelo guardado: Nueva mejor perplejidad: {self.min_score:.4f}\")\n",
    "            self.patience_counter = 0\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "            print(f\"Perplejidad no mejoró. Paciencia: {self.patience_counter}/{self.patience}\")\n",
    "            if self.patience_counter >= self.patience: # Usar >= por si acaso\n",
    "                print(\"Deteniendo el entrenamiento (Early Stopping)...\")\n",
    "                self.model.stop_training = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HBZIwR0gruA"
   },
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "oQq1PHDkxDvN",
    "outputId": "827a9ce2-f66c-4d16-aec4-f6e966d17cab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.2217 - loss: 2.8668\n",
      " Perplejidad promedio: 8.4758 \n",
      "\n",
      "Modelo guardado: Nueva mejor perplejidad: 8.4758\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1394s\u001b[0m 3s/step - accuracy: 0.2218 - loss: 2.8662\n",
      "Epoch 2/20\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 986ms/step - accuracy: 0.3736 - loss: 2.1632\n",
      " Perplejidad promedio: 6.9540 \n",
      "\n",
      "Modelo guardado: Nueva mejor perplejidad: 6.9540\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1368s\u001b[0m 3s/step - accuracy: 0.3737 - loss: 2.1631\n",
      "Epoch 3/20\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 974ms/step - accuracy: 0.4142 - loss: 1.9884\n",
      " Perplejidad promedio: 6.2380 \n",
      "\n",
      "Modelo guardado: Nueva mejor perplejidad: 6.2380\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1354s\u001b[0m 3s/step - accuracy: 0.4142 - loss: 1.9883\n",
      "Epoch 4/20\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 976ms/step - accuracy: 0.4410 - loss: 1.8832\n",
      " Perplejidad promedio: 5.7209 \n",
      "\n",
      "Modelo guardado: Nueva mejor perplejidad: 5.7209\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1362s\u001b[0m 3s/step - accuracy: 0.4410 - loss: 1.8831\n",
      "Epoch 5/20\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975ms/step - accuracy: 0.4583 - loss: 1.8136\n",
      " Perplejidad promedio: 5.4343 \n",
      "\n",
      "Modelo guardado: Nueva mejor perplejidad: 5.4343\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1359s\u001b[0m 3s/step - accuracy: 0.4583 - loss: 1.8136\n",
      "Epoch 6/20\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 976ms/step - accuracy: 0.4712 - loss: 1.7605\n",
      " Perplejidad promedio: 5.1760 \n",
      "\n",
      "Modelo guardado: Nueva mejor perplejidad: 5.1760\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1357s\u001b[0m 3s/step - accuracy: 0.4712 - loss: 1.7604\n",
      "Epoch 7/20\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975ms/step - accuracy: 0.4824 - loss: 1.7093\n",
      " Perplejidad promedio: 4.9845 \n",
      "\n",
      "Modelo guardado: Nueva mejor perplejidad: 4.9845\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1362s\u001b[0m 3s/step - accuracy: 0.4824 - loss: 1.7092\n",
      "Epoch 8/20\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998ms/step - accuracy: 0.4927 - loss: 1.6692\n",
      " Perplejidad promedio: 4.7605 \n",
      "\n",
      "Modelo guardado: Nueva mejor perplejidad: 4.7605\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1382s\u001b[0m 3s/step - accuracy: 0.4927 - loss: 1.6692\n",
      "Epoch 9/20\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5013 - loss: 1.6364\n",
      " Perplejidad promedio: 4.6215 \n",
      "\n",
      "Modelo guardado: Nueva mejor perplejidad: 4.6215\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1395s\u001b[0m 3s/step - accuracy: 0.5013 - loss: 1.6364\n",
      "Epoch 10/20\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5087 - loss: 1.6078\n",
      " Perplejidad promedio: 4.5393 \n",
      "\n",
      "Modelo guardado: Nueva mejor perplejidad: 4.5393\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2296s\u001b[0m 4s/step - accuracy: 0.5087 - loss: 1.6078\n",
      "Epoch 11/20\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5151 - loss: 1.5827\n",
      " Perplejidad promedio: 4.4681 \n",
      "\n",
      "Modelo guardado: Nueva mejor perplejidad: 4.4681\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1384s\u001b[0m 3s/step - accuracy: 0.5151 - loss: 1.5827\n",
      "Epoch 12/20\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5204 - loss: 1.5616\n",
      " Perplejidad promedio: 4.4288 \n",
      "\n",
      "Modelo guardado: Nueva mejor perplejidad: 4.4288\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1409s\u001b[0m 3s/step - accuracy: 0.5204 - loss: 1.5616\n",
      "Epoch 13/20\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.5247 - loss: 1.5440\n",
      " Perplejidad promedio: 4.3787 \n",
      "\n",
      "Modelo guardado: Nueva mejor perplejidad: 4.3787\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4328s\u001b[0m 8s/step - accuracy: 0.5247 - loss: 1.5440\n",
      "Epoch 14/20\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5284 - loss: 1.5291\n",
      " Perplejidad promedio: 4.3763 \n",
      "\n",
      "Modelo guardado: Nueva mejor perplejidad: 4.3763\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1382s\u001b[0m 3s/step - accuracy: 0.5284 - loss: 1.5291\n",
      "Epoch 15/20\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5313 - loss: 1.5171\n",
      " Perplejidad promedio: 4.3419 \n",
      "\n",
      "Modelo guardado: Nueva mejor perplejidad: 4.3419\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1365s\u001b[0m 3s/step - accuracy: 0.5313 - loss: 1.5171\n",
      "Epoch 16/20\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 977ms/step - accuracy: 0.5338 - loss: 1.5068\n",
      " Perplejidad promedio: 4.3190 \n",
      "\n",
      "Modelo guardado: Nueva mejor perplejidad: 4.3190\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1352s\u001b[0m 3s/step - accuracy: 0.5338 - loss: 1.5068\n",
      "Epoch 17/20\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988ms/step - accuracy: 0.5361 - loss: 1.4974\n",
      " Perplejidad promedio: 4.3307 \n",
      "\n",
      "Perplejidad no mejoró. Paciencia: 1/5\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1356s\u001b[0m 3s/step - accuracy: 0.5361 - loss: 1.4974\n",
      "Epoch 18/20\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 984ms/step - accuracy: 0.5381 - loss: 1.4896\n",
      " Perplejidad promedio: 4.3396 \n",
      "\n",
      "Perplejidad no mejoró. Paciencia: 2/5\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1386s\u001b[0m 3s/step - accuracy: 0.5381 - loss: 1.4896\n",
      "Epoch 19/20\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5399 - loss: 1.4825\n",
      " Perplejidad promedio: 4.2849 \n",
      "\n",
      "Modelo guardado: Nueva mejor perplejidad: 4.2849\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1395s\u001b[0m 3s/step - accuracy: 0.5399 - loss: 1.4825\n",
      "Epoch 20/20\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5414 - loss: 1.4768\n",
      " Perplejidad promedio: 4.3285 \n",
      "\n",
      "Perplejidad no mejoró. Paciencia: 1/5\n",
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1393s\u001b[0m 3s/step - accuracy: 0.5414 - loss: 1.4768\n"
     ]
    }
   ],
   "source": [
    "# fiteamos, nótese el agregado del callback con su inicialización. El batch_size lo podemos seleccionar a mano\n",
    "# en general, lo mejor es escoger el batch más grande posible que minimice el tiempo de cada época.\n",
    "# En la variable `history_ppl` se guardarán los valores de perplejidad para cada época.\n",
    "history_ppl = []\n",
    "hist = model.fit(X, y, epochs=20,\n",
    "                 callbacks=[PplCallback(tokenized_sentences_val,history_ppl)],\n",
    "                 batch_size=2128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "K30JHB3Dv-mx",
    "outputId": "80551a90-832f-4b30-e7cb-ccda4f8d4f0a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(59431) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Entrenamiento\n",
    "epoch_count = range(1, len(history_ppl) + 1)\n",
    "sns.lineplot(x=epoch_count,  y=history_ppl)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "Rhy5hZN38qfO",
    "outputId": "ba5bd18f-b88d-4de2-8155-02d4812cb51e"
   },
   "outputs": [],
   "source": [
    "# Cargamos el mejor modelo guardado del entrenamiento para hacer inferencia\n",
    "model = keras.models.load_model('my_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KN6Fg_BsxJe6"
   },
   "source": [
    "\n",
    "### Predicción del próximo caracter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "IBvKHFPmzpy2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(59432) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "# Se puede usar gradio para probar el modelo\n",
    "# Gradio es una herramienta muy útil para crear interfaces para ensayar modelos\n",
    "# https://gradio.app/\n",
    "\n",
    "!pip install -q gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNyBykvhzs7-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(59439) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def model_response(human_text):\n",
    "\n",
    "    # Encodeamos\n",
    "    encoded = [char2idx[ch] for ch in human_text.lower() ]\n",
    "    # Si tienen distinto largo\n",
    "    encoded = pad_sequences([encoded], maxlen=max_context_size, padding='pre')\n",
    "\n",
    "    # Predicción softmax\n",
    "    y_hat = np.argmax(model.predict(encoded)[0,-1,:])\n",
    "\n",
    "\n",
    "    # Debemos buscar en el vocabulario el caracter\n",
    "    # que corresopnde al indice (y_hat) predicho por le modelo\n",
    "    out_word = ''\n",
    "    out_word = idx2char[y_hat]\n",
    "\n",
    "    # Agrego la palabra a la frase predicha\n",
    "    return human_text + out_word\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=model_response,\n",
    "    inputs=[\"textbox\"],\n",
    "    outputs=\"text\")\n",
    "\n",
    "iface.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCeMWWupxN1-"
   },
   "source": [
    "### Generación de secuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwbS_pfhxvB3"
   },
   "outputs": [],
   "source": [
    "def generate_seq(model, seed_text, max_length, n_words):\n",
    "    \"\"\"\n",
    "        Exec model sequence prediction\n",
    "\n",
    "        Args:\n",
    "            model (keras): modelo entrenado\n",
    "            seed_text (string): texto de entrada (input_seq)\n",
    "            max_length (int): máxima longitud de la sequencia de entrada\n",
    "            n_words (int): números de caracteres a agregar a la sequencia de entrada\n",
    "        returns:\n",
    "            output_text (string): sentencia con las \"n_words\" agregadas\n",
    "    \"\"\"\n",
    "    output_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "\t\t# Encodeamos\n",
    "        encoded = [char2idx[ch] for ch in output_text.lower() ]\n",
    "\t\t# Si tienen distinto largo\n",
    "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "\n",
    "\t\t# Predicción softmax\n",
    "        y_hat = np.argmax(model.predict(encoded,verbose=0)[0,-1,:])\n",
    "\t\t# Vamos concatenando las predicciones\n",
    "        out_word = ''\n",
    "\n",
    "        out_word = idx2char[y_hat]\n",
    "\n",
    "\t\t# Agrego las palabras a la frase predicha\n",
    "        output_text += out_word\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoFqRC5pxzqS"
   },
   "outputs": [],
   "source": [
    "input_text='habia una vez'\n",
    "\n",
    "generate_seq(model, input_text, max_length=max_context_size, n_words=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drJ6xn5qW1Hl"
   },
   "source": [
    "###  Beam search y muestreo aleatorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vovn9XZW1Hl"
   },
   "outputs": [],
   "source": [
    "# funcionalidades para hacer encoding y decoding\n",
    "\n",
    "def encode(text,max_length=max_context_size):\n",
    "\n",
    "    encoded = [char2idx[ch] for ch in text]\n",
    "    encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "\n",
    "    return encoded\n",
    "\n",
    "def decode(seq):\n",
    "    return ''.join([idx2char[ch] for ch in seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_lZiQwkW1Hl"
   },
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "# función que selecciona candidatos para el beam search\n",
    "def select_candidates(pred,num_beams,vocab_size,history_probs,history_tokens,temp,mode):\n",
    "\n",
    "  # colectar todas las probabilidades para la siguiente búsqueda\n",
    "  pred_large = []\n",
    "\n",
    "  for idx,pp in enumerate(pred):\n",
    "    pred_large.extend(np.log(pp+1E-10)+history_probs[idx])\n",
    "\n",
    "  pred_large = np.array(pred_large)\n",
    "\n",
    "  # criterio de selección\n",
    "  if mode == 'det':\n",
    "    idx_select = np.argsort(pred_large)[::-1][:num_beams] # beam search determinista\n",
    "  elif mode == 'sto':\n",
    "    idx_select = np.random.choice(np.arange(pred_large.shape[0]), num_beams, p=softmax(pred_large/temp)) # beam search con muestreo aleatorio\n",
    "  else:\n",
    "    raise ValueError(f'Wrong selection mode. {mode} was given. det and sto are supported.')\n",
    "\n",
    "  # traducir a índices de token en el vocabulario\n",
    "  new_history_tokens = np.concatenate((np.array(history_tokens)[idx_select//vocab_size],\n",
    "                        np.array([idx_select%vocab_size]).T),\n",
    "                      axis=1)\n",
    "\n",
    "  # devolver el producto de las probabilidades (log) y la secuencia de tokens seleccionados\n",
    "  return pred_large[idx_select.astype(int)], new_history_tokens.astype(int)\n",
    "\n",
    "\n",
    "def beam_search(model,num_beams,num_words,input,temp=1,mode='det'):\n",
    "\n",
    "    # first iteration\n",
    "\n",
    "    # encode\n",
    "    encoded = encode(input)\n",
    "\n",
    "    # first prediction\n",
    "    y_hat = model.predict(encoded,verbose=0)[0,-1,:]\n",
    "\n",
    "    # get vocabulary size\n",
    "    vocab_size = y_hat.shape[0]\n",
    "\n",
    "    # initialize history\n",
    "    history_probs = [0]*num_beams\n",
    "    history_tokens = [encoded[0]]*num_beams\n",
    "\n",
    "    # select num_beams candidates\n",
    "    history_probs, history_tokens = select_candidates([y_hat],\n",
    "                                        num_beams,\n",
    "                                        vocab_size,\n",
    "                                        history_probs,\n",
    "                                        history_tokens,\n",
    "                                        temp,\n",
    "                                        mode)\n",
    "\n",
    "    # beam search loop\n",
    "    for i in range(num_words-1):\n",
    "\n",
    "      preds = []\n",
    "\n",
    "      for hist in history_tokens:\n",
    "\n",
    "        # actualizar secuencia de tokens\n",
    "        input_update = np.array([hist[i+1:]]).copy()\n",
    "\n",
    "        # predicción\n",
    "        y_hat = model.predict(input_update,verbose=0)[0,-1,:]\n",
    "\n",
    "        preds.append(y_hat)\n",
    "\n",
    "      history_probs, history_tokens = select_candidates(preds,\n",
    "                                                        num_beams,\n",
    "                                                        vocab_size,\n",
    "                                                        history_probs,\n",
    "                                                        history_tokens,\n",
    "                                                        temp,\n",
    "                                                        mode)\n",
    "\n",
    "    return history_tokens[:,-(len(input)+num_words):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GeLqAoOYW1Hm"
   },
   "outputs": [],
   "source": [
    "# predicción con beam search\n",
    "salidas = beam_search(model,num_beams=10,num_words=20,input=\"habia una vez\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8HQoLhw-NYg"
   },
   "outputs": [],
   "source": [
    "salidas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2S3_I3S1W1Hm"
   },
   "outputs": [],
   "source": [
    "# veamos las salidas\n",
    "decode(salidas[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_LlqmtEW1Hn"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Perplejidad\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.lineplot(x=range(1, len(history_ppl)+1), y=history_ppl)\n",
    "plt.title(\"Perplejidad por época\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Perplejidad\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.lineplot(x=range(1, len(hist.history['accuracy'])+1), y=hist.history['accuracy'], label='Train accuracy')\n",
    "plt.title(\"Precisión por época\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
